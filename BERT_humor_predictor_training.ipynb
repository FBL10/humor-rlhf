{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSvzxLWKBiYF"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgAelrkwbdoZ"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import shutil\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unzip the dataset\n",
        "zip_path = '/content/humor-detection.zip'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/data')\n",
        "\n",
        "# Check for extracted files\n",
        "for dirname, _, filenames in os.walk('/content/data'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Load data\n",
        "train_x = pd.read_pickle(\"/content/data/X_train.pickle\")\n",
        "train_y = pd.read_pickle(\"/content/data/y_train.pickle\")\n",
        "test_x = pd.read_pickle(\"/content/data/X_test.pickle\")\n",
        "test_y = pd.read_pickle(\"/content/data/y_test.pickle\")\n",
        "\n",
        "# Split data into training and validation sets\n",
        "def train_val_split(train_x, train_y):\n",
        "    out_train_x, out_train_y, out_val_x, out_val_y = [], [], [], []\n",
        "    for i in range(len(train_x)):\n",
        "        if random.random() < 0.8:\n",
        "            out_train_x.append(train_x[i])\n",
        "            out_train_y.append(train_y[i])\n",
        "        else:\n",
        "            out_val_x.append(train_x[i])\n",
        "            out_val_y.append(train_y[i])\n",
        "    return out_train_x, out_train_y, out_val_x, out_val_y\n",
        "\n",
        "train_x, train_y, val_x, val_y = train_val_split(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAx-XpTUBiYK",
        "outputId": "22096f14-1c3e-49c8-bbcf-3557b6eb6986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I8DfjEPV8Tb",
        "outputId": "a8f574bb-0335-442d-ee03-9696d0cfbc1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDDBMYG2BiYL"
      },
      "outputs": [],
      "source": [
        "def lemmatize(s):\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    return \" \".join([wordnet_lemmatizer.lemmatize(w, 'v') for w in s.split(\" \")])\n",
        "\n",
        "def lower(s):\n",
        "    return s.lower()\n",
        "\n",
        "def clean(data):\n",
        "    for item in data:\n",
        "        lemmatize(item)\n",
        "        lower(item)\n",
        "        re.sub(r'\\d+', '', item)  # Remove numbers\n",
        "    return data\n",
        "\n",
        "def tokenize(text):\n",
        "    return tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "def process(data):\n",
        "    cleaned = clean(data)\n",
        "    return tokenize(cleaned)\n",
        "\n",
        "train_batch = process(train_x)\n",
        "test_batch = process(test_x)\n",
        "val_batch = process(val_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnhDzSVBBiYM"
      },
      "outputs": [],
      "source": [
        "# Create a custom dataset class\n",
        "class HumorDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}, torch.tensor(self.labels[idx])\n",
        "\n",
        "# Create DataLoader objects\n",
        "train_dataset = HumorDataset(train_batch, train_y)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "train_dataset = HumorDataset(train_batch, train_y)\n",
        "val_dataset = HumorDataset(val_batch, val_y)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score\n",
        "import torch\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "batch_losses = []\n",
        "epoch_losses = []\n",
        "val_precisions = []\n",
        "\n",
        "# Training loop with validation\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # Example optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Example loss function\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    # Training loop\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        # Unpack inputs and labels\n",
        "        inputs, labels = batch\n",
        "        if isinstance(inputs, dict):  # Handle tokenized input format\n",
        "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "        else:\n",
        "            inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass, backward pass, and optimization step\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**inputs) if isinstance(inputs, dict) else model(inputs)\n",
        "        loss = criterion(outputs.logits if hasattr(outputs, \"logits\") else outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store and log batch loss\n",
        "        batch_losses.append(loss.item())\n",
        "        total_loss += loss.item()\n",
        "        if (batch_idx + 1) % 25 == 0:\n",
        "          print(f\"\\tBatch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Calculate and store epoch loss\n",
        "    avg_epoch_loss = total_loss / len(train_loader)\n",
        "    epoch_losses.append(avg_epoch_loss)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for val_batch in val_loader:\n",
        "            val_inputs, val_labels = val_batch\n",
        "            if isinstance(val_inputs, dict):\n",
        "                val_inputs = {key: val.to(device) for key, val in val_inputs.items()}\n",
        "            else:\n",
        "                val_inputs = val_inputs.to(device)\n",
        "            val_labels = val_labels.to(device)\n",
        "\n",
        "            val_outputs = model(**val_inputs) if isinstance(val_inputs, dict) else model(val_inputs)\n",
        "            val_loss += criterion(val_outputs.logits if hasattr(val_outputs, \"logits\") else val_outputs, val_labels).item()\n",
        "\n",
        "            preds = torch.argmax(val_outputs.logits if hasattr(val_outputs, \"logits\") else val_outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(val_labels.cpu().numpy())\n",
        "\n",
        "    # Calculate and store validation precision\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    val_precisions.append(precision)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} Loss: {avg_epoch_loss:.4f}, Validation Precision: {precision:.4f}\")"
      ],
      "metadata": {
        "id": "l_-rBXmeEvBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot batch losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(batch_losses, label='Batch Loss')\n",
        "plt.xlabel('Batch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Batch Loss During Training')\n",
        "plt.legend()\n",
        "plt.savefig('batch_loss_plot.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot validation precision\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(val_precisions, label='Validation Precision', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Validation Precision Over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig('validation_precision_plot.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KMwu61umfYuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMO6I3YEBiYN",
        "outputId": "4d427170-daf7-4cd9-fa0e-20b20e2f4e02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/teamspace/studios/this_studio/humor_model/tokenizer_config.json',\n",
              " '/teamspace/studios/this_studio/humor_model/special_tokens_map.json',\n",
              " '/teamspace/studios/this_studio/humor_model/vocab.txt',\n",
              " '/teamspace/studios/this_studio/humor_model/added_tokens.json',\n",
              " '/teamspace/studios/this_studio/humor_model/tokenizer.json')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model\n",
        "output_dir = \"/teamspace/studios/this_studio/humor_model\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcFmhEeKBiYO",
        "outputId": "8252a3cd-0612-460e-8baa-f5c021c6d796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zipped humor_model into humor_model.zip\n"
          ]
        }
      ],
      "source": [
        "# Zip and download the model\n",
        "directory_to_zip = \"humor_model\"\n",
        "output_zip_file = \"humor_model.zip\"\n",
        "shutil.make_archive(output_zip_file.replace(\".zip\", \"\"), 'zip', directory_to_zip)\n",
        "print(f\"Zipped {directory_to_zip} into {output_zip_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}